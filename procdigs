#!/usr/bin/env python

import sys
import glob
import re
import wave
import time
import csv
import subprocess
import hashlib
from os import path
from tqdm import tqdm
from multiprocessing import Pool


def main():

	path = './'
	if len(sys.argv) > 1:
		path = sys.argv[1]

	fieldnames = [
		'path',
		'id',
		'side',
		'dig',
		'take',
		'notes',
		'channels',
		'sample_rate',
		'bitrate',
		'encoding',
		'precision',
		'duration',
		'size',
		'md5',
		'updated at (UTC)',

	]

	csv_writer = csv.DictWriter(sys.stdout, fieldnames = fieldnames, quoting=csv.QUOTE_MINIMAL)
	csv_writer.writeheader()

	# for this_path in paths:
	# 	file_paths = glob.glob(this_path + '/*.wav')
	# 	for file_path in tqdm(file_paths):
	# 		file_entries = get_file_information(file_path)
	# 		for file_entry in file_entries:
	# 			csv_writer.writerow(file_entry)
	file_paths = glob.glob(path + '/*.wav')

	pool = Pool(8)
	file_entries = list(tqdm(pool.imap(get_file_information, file_paths), total=len(file_paths)))
	# csv_writer.writerows(file_entries)
	for file_entry in file_entries:		
		csv_writer.writerows(file_entry)
		# for entry in file_entry:
		# 	csv_writer.writerow(entry)

# def get_soxi_line(reg, input):
# 	return re.sub(reg, '', input)

def soxi_get(file_path, flag):
	return subprocess.check_output(['soxi', flag, file_path]).strip(" \n\t\r")

def soxi_get_channels(file_path):
	return soxi_get(file_path, '-c')

def soxi_get_sample_rate(file_path):
	return soxi_get(file_path, '-r')

def soxi_get_precision(file_path):
	return soxi_get(file_path, '-b')

def soxi_get_duration(file_path):
	return soxi_get(file_path, '-d')

def soxi_get_bitrate(file_path):
	return soxi_get(file_path, '-B')

def soxi_get_encoding(file_path):
	return soxi_get(file_path, '-e')

def get_file_intrinsic_information(file_path):
	# file_info = sox.file_info.info(file_path)
	# {'channels': 2, 'sample_rate': 96000.0, 'bitrate': 24, 'duration': 5822.848, 'num_samples': 558993408, 'encoding': 'Signed Integer PCM', 'silent': False}
	result = {
		'channels': soxi_get_channels(file_path),
		'sample_rate': soxi_get_sample_rate(file_path),
		'precision': soxi_get_precision(file_path),
		'duration': soxi_get_duration(file_path),
		'bitrate': soxi_get_bitrate(file_path),
		'encoding': soxi_get_encoding(file_path)
	}

	return result;

	# info_line = re.compile('(.*?):(.+)')
	# output = subprocess.check_output(['soxi', file_path]).split("\n")
	# replace_header = re.compile('.*?: ')

	# result = {}

	# if len(output) >= 9:
	# 	result = {
	# 		'channels': get_soxi_line(replace_header, output[2]), #channels
	# 		'sample_rate': get_soxi_line(replace_header, output[3]), #sample rate
	# 		'precision': get_soxi_line(replace_header, output[4]), #precision
	# 		'duration': re.sub('\s=.+$', '', get_soxi_line(replace_header, output[5])), #duration
	# 		'bitrate': get_soxi_line(replace_header, output[7]), #bit rate
	# 		'encoding': get_soxi_line(replace_header, output[8]) #encoding
	# 	}
	# else:
	# 	print "ERROR: " + file_path
	# return result


def get_file_extrinsic_information(file_path):
	# regular expression: (path) (id) (side) (digg unit) (take) (notes)
	m = re.match('^(?:.+\/)(.+?)-([ab])-(d\d+)-(t\d+)(?:-(.+))?.wav$', file_path)
	file_entry = {}
	if m:
		file_entry = {
			'path': path.abspath(file_path),
			'id': m.group(1),
			'side': m.group(2),
			'dig': m.group(3),
			'take': m.group(4),
			'notes': m.group(5),
			'updated at (UTC)': time.strftime("%d %b %Y %H:%M:%S", time.gmtime(path.getmtime(file_path))),
			'size': get_size(path.getsize(file_path)),
			'md5': md5(file_path)
		}
	else :
		file_name_regex = re.match('^(?:.+/)(.+?).wav$', file_path)
		file_entry = {
			'path': path.abspath(file_path),
			'id': file_name_regex.group(1),
			'updated at (UTC)': time.strftime("%d %b %Y %H:%M:%S", time.gmtime(path.getmtime(file_path))),
			'size': get_size(path.getsize(file_path)),
			'md5': md5(file_path)
		}

	return file_entry


# from https://stackoverflow.com/questions/3431825/generating-an-md5-checksum-of-a-file
def md5(fname):
	hash_md5 = hashlib.md5()
	with open(fname, "rb") as f:
		for chunk in iter(lambda: f.read(4096), b""):
			hash_md5.update(chunk)
	return hash_md5.hexdigest()


def get_file_information(file_path):
	result = []
	file_extrinsic_information = get_file_extrinsic_information(file_path)
	file_intrinsic_information = get_file_intrinsic_information(file_path)
	# split file_extrinsic_information id by special character "_" and
	# enter each id modified in the result
	ids = file_extrinsic_information['id'].split('_')

	for id in ids:
		result_line = dict(file_intrinsic_information, **file_extrinsic_information)
		result_line['id'] = id
		result.append(result_line)

	return result


# from https://stackoverflow.com/questions/1094841/reusable-library-to-get-human-readable-version-of-file-size
def get_size(num, suffix='B'):
	for unit in ['','K','M','G','T','P','E','Z']:
		if abs(num) < 1024.0:
			return "%3.2f%s%s" % (num, unit, suffix)
		num /= 1024.0
	return "%.1f%s%s" % (num, 'Yi', suffix)

if __name__ == "__main__":
	main()
