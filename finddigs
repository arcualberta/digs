#!/usr/bin/env python

import csv
import sys
import os
import httplib2
import json
import argparse

from apiclient import discovery
from oauth2client import client
from oauth2client import tools
from oauth2client.file import Storage

def main():	
	find_digs()


def find_digs():
	
	# Move to Configuration Sheet
	ID_RANGE = 'Audio Cassettes!A2:B'
	PREV_DATA_RANGE = 'Audio Cassettes!L2:M'
	RESULTS_RANGE = 'Audio Cassettes!C2:M'
	EXCEPTIONS_RANGE = 'Exceptions!A2:M'

	flags = get_flags()

	if flags:
		spreadsheet_id = flags.id
		processed_file_path = flags.processed
		
		service = get_service(flags)
		pivot_file = get_inventory_data(
			service = service, spreadsheet_id = spreadsheet_id, 
			range = ID_RANGE
		)
		# prev_data is an array of strings in json format describing files already
		# found
		prev_data = get_inventory_data(
			service = service, spreadsheet_id = spreadsheet_id, 
			range = PREV_DATA_RANGE
		)
		
		processed_file = get_file_dictionary(processed_file_path)
		existing_files = get_existing_files(processed_file)
		results, exceptions = process_all_file_entries(
								pivot_file, prev_data, existing_files
							)

		data = get_data(
			results_range=RESULTS_RANGE, 
			results=results,
			exceptions_range=EXCEPTIONS_RANGE,
			exceptions=exceptions
		)

		clear_exceptions(service, spreadsheet_id, EXCEPTIONS_RANGE)
		set_processed_inventory_data(service, spreadsheet_id, data)


def get_data(results_range, results, exceptions_range, exceptions):
	data = [	
			{
				'range': results_range,
				'majorDimension': 'ROWS',
				'values': results,
			},
			{
				'range': exceptions_range,
				'majorDimension': 'ROWS',
				'values': exceptions
			}
		]
	return data

def get_flags():
	flags = None
	try:
		parser = argparse.ArgumentParser(parents=[tools.argparser])
		parser.add_argument(
			'-i', '--id', required=True,
			help="Id to specify which google spreadsheet to use"
		)
		parser.add_argument(
			'-p', '--processed-path', dest='processed', required=True,
			help="Path to CSV file containing digs information"
		)
		flags = parser.parse_args()
	except ImportError:
		flags = None

	return flags

def clear_exceptions(service, spreadsheet_id, exceptions_range):
	service.spreadsheets().values().clear(
		spreadsheetId=spreadsheet_id, 
		range=exceptions_range,
		body={}
	).execute()

def set_processed_inventory_data(service, spreadsheet_id, data):
	body = {
		'data': data,
		'valueInputOption': "RAW",		
	}

	response = service.spreadsheets().values().batchUpdate(
		spreadsheetId = spreadsheet_id, 
		body=body
	).execute()

def process_all_file_entries(pivot_file, prev_data, existing_files):
	
	result = []
	pivot_file_count = len(pivot_file)
	progress(0, pivot_file_count)
	blank_row = [''] * 11
	
	for index, file in enumerate(pivot_file):
		if len(file) >= 2:
			media_id = file[0]
			side = normalized_side(file[1])
			if media_id != '':
				key = media_id + '-' + side
				existing = pop_existing_files(existing_files, key)
				# check if already exists
				result.append(add_new_files(prev_data, index, existing))
			else:
				result.append(blank_row)
		else:
			result.append(blank_row)
		progress(index+1, pivot_file_count)
	print '\n'

	# existing files has the files not included in inventory (exceptions)
	return result, prepare_exceptions(existing_files)


def prepare_exceptions(files):
	result = []
	entries = flatten_dcitionary(files)
	for entry in entries:
		for file in entry:
			line = get_file_information(file)
			name = file['id'] + '-' + file['side'] + '-' + file['dig'] + '-' + file['take'] + '.wav'
			test = [file["path"], name]
			

			result.append(test + line)
	return result

def flatten_dcitionary(dictionary):
	result = []
	for key in dictionary:
		result.append(dictionary[key])
	return result

def add_new_files(prev_data, index, existing):

	result = existing

	if index < len(prev_data):
		prev_datum = json.loads(str(prev_data[index][0]))
		for prev_file in prev_datum:
			found = False
			for file in result:
				if file["md5"] == prev_file["md5"]:
					found = True
					break

			if not found:
				result.append(prev_file)

	result_count = len(result)

	information_row = [''] * 9

	if result_count == 1:
		information_row = get_file_information(result[0])		

	return information_row + [json.dumps(result), result_count];


def get_file_information(file):
	result = [''] * 9
	result[0] = file['id'] + '-' + file['side'] + '.wav'
	result[2] = file['channels']
	result[3] = file['sample_rate']
	result[4] = file['precision']
	result[5] = file['duration']
	result[6] = file['size']
	result[7] = file['bitrate']
	result[8] = file['encoding']
	return result


def get_service(flags):
	credentials = get_credentials(flags)
	http = credentials.authorize(httplib2.Http())
	discovery_url = ('https://sheets.googleapis.com/$discovery/rest?'
					 'version=v4')
	service = discovery.build(
		'sheets', 'v4', http=http, discoveryServiceUrl=discovery_url
	)

	return service


def get_inventory_data(service, spreadsheet_id, range):

	result = service.spreadsheets().values().get(
        spreadsheetId=spreadsheet_id, range=range).execute()
	values = result.get('values', [])

	if not values:
		return []
	else:
		return values


# from https://developers.google.com/sheets/api/quickstart/python
def get_credentials(flags):

	SCOPES = 'https://www.googleapis.com/auth/spreadsheets'
	CLIENT_SECRET_FILE = 'client_secret.json'
	APPLICATION_NAME = 'Google Sheets API Python Quickstart'

	home_dir = os.path.expanduser('~')
	credential_dir = os.path.join(home_dir, '.credentials')
	if not os.path.exists(credential_dir):
		os.makedirs(credential_dir)
	credential_path = os.path.join(credential_dir,
		'sheets.googleapis.com-python-quickstart.json')

	store = Storage(credential_path)
	credentials = store.get()
	if not credentials or credentials.invalid:
		flow = client.flow_from_clientsecrets(CLIENT_SECRET_FILE, SCOPES)
		flow.user_agent = APPLICATION_NAME
		if flags:
			credentials = tools.run_flow(flow, store, flags)
		else: # Needed only for compatibility with Python 2.6
			credentials = tools.run(flow, store)
		print('Storing credentials to ' + credential_path)
	return credentials

def get_file_dictionary(file_path):
	with open(file_path) as csv_file:
		reader = csv.DictReader(csv_file)
		return list(reader)


def get_existing_files(processed_file):
	result = {}
	for file in processed_file:
		key = file['id'] + '-' + file['side']
		if key not in result:
			result[key] = []
		result[key].append(file)
	return result


def normalized_side(x):
	return {
		'b': 'b',
		'2': 'b',
	}.get(x.lower(), 'a')


def pop_existing_files(existing_files, key):
	if key not in existing_files:
		return []
	else:
		return existing_files.pop(key)


# From https://gist.github.com/vladignatyev/06860ec2040cb497f0f3
def progress(count, total, status=''):
	bar_len = 60
	filled_len = int(round(bar_len * count / float(total)))

	percents = round(100.0 * count / float(total), 1)
	bar = '#' * filled_len + '-' * (bar_len - filled_len)

	sys.stdout.write('[%s] %s%s ...%s\r' % (bar, percents, '%', status))
	sys.stdout.flush()


if __name__ == '__main__':
	main()