#!/usr/bin/env python

import csv
import sys
import os
import httplib2
import json
import argparse
from apiclient import discovery
from oauth2client import client
from oauth2client import tools
from oauth2client.file import Storage
from tqdm import tqdm

def main():
	find_digs()


def find_digs():
	flags = get_flags()
	EXCEPTIONS_RANGE = 'Exceptions!A2:N'
	REPORT_RANGE = 'Report!A2:E'

	if flags:
		service = get_service(flags)
		# work_sheet = 'Audio Cassettes'
		work_sheets = get_work_sheets(service, flags)
		clear_exceptions(service, flags.id, EXCEPTIONS_RANGE, REPORT_RANGE)
		progress = tqdm(work_sheets)
		progress.set_description('Work sheets')
		for work_sheet in progress:
			process_worksheet(flags, service, str(work_sheet[0]), EXCEPTIONS_RANGE)
		print '\n'


def get_work_sheets(service, flags):
	WORK_SHEETS_RANGE = 'Configuration!A2:A'
	spreadsheet_id = flags.id

	result = service.spreadsheets().values().get(
        spreadsheetId=spreadsheet_id, range=WORK_SHEETS_RANGE).execute()
	return result.get('values', [])


def set_worksheet_headers(service, flags, work_sheet):
	headers = [
		['Digitized file name'],
		['Digitization notes'],
		['Channels'],
		['Sample rate'],
		['Precision'],
		['Duration HH:MM:SS'],
		['Size'],
		['Bit rate'],
		['Encoding'],
		['Data'],
		['File count'],
		['Repeating MD5']
	]

	body = {
		'values': headers,
		'majorDimension': 'COLUMNS'
	}

	range = work_sheet + '!C1:N1'

	response = service.spreadsheets().values().update(
		spreadsheetId = flags.id,
		range = range,
		body = body,
		valueInputOption = 'RAW',
	).execute()


def process_worksheet(flags, service, work_sheet, exceptions_range):

	# EXCEPTIONS_RANGE = 'Exceptions!A2:M'
	ID_RANGE = work_sheet + '!A2:B'
	PREV_DATA_RANGE = work_sheet + '!L2:M'
	RESULTS_RANGE = work_sheet + '!C2:N'
	spreadsheet_id = flags.id
	processed_file_path = flags.processed

	set_worksheet_headers(service, flags, work_sheet)

	pivot_file = get_inventory_data(
		service = service, spreadsheet_id = spreadsheet_id,
		range = ID_RANGE
	)

	# prev_data is an array of strings in json format describing
	# files already found

	prev_data = []
	if not flags.resolve:
		prev_data = get_inventory_data(
			service = service, spreadsheet_id = spreadsheet_id,
			range = PREV_DATA_RANGE
		)

	processed_file = get_file_dictionary(processed_file_path)
	existing_files = get_existing_files(processed_file)
	results, exceptions, report = process_all_file_entries(
							work_sheet, pivot_file, prev_data, existing_files
						)

	data = get_data(
		results_range=RESULTS_RANGE,
		results=results,
		exceptions_range=exceptions_range,
		exceptions=exceptions,
		report_range='Report!A2:E',
		report=report
	)

	#XXX switch vs clear exceptions and add exception based on flag

	# clear_exceptions(service, spreadsheet_id, EXCEPTIONS_RANGE)
	set_processed_inventory_data(service, spreadsheet_id, data)


def get_data(results_range, results, exceptions_range, exceptions, report_range, report):
	return [
			{
				'range': results_range,
				'majorDimension': 'ROWS',
				'values': results,
			},
			{
				'range': exceptions_range,
				'majorDimension': 'ROWS',
				'values': exceptions
			},{
				'range': report_range,
				'majorDimension': 'ROWS',
				'values': report
			}
		]


def get_flags():
	flags = None
	try:
		parser = argparse.ArgumentParser(parents=[tools.argparser])
		parser.add_argument(
			'-i', '--id', required=True,
			help="Id to specify which google spreadsheet to use"
		)
		parser.add_argument(
			'-p', '--processed-path', dest='processed', required=True,
			help="Path to CSV file containing digs information"
		)
		parser.add_argument(
			'-r', '--resolve', action='store_true', default=False,
			help="Recreate data row ignoring previous files"
		)

		flags = parser.parse_args()
	except ImportError:
		flags = None

	return flags


def clear_exceptions(service, spreadsheet_id, exceptions_range, report_range):
	# if does not exist create it
	service.spreadsheets().values().clear(
		spreadsheetId=spreadsheet_id,
		range=exceptions_range,
		body={}
	).execute()

	service.spreadsheets().values().clear(
		spreadsheetId=spreadsheet_id,
		range=report_range,
		body={}
	).execute()

def set_processed_inventory_data(service, spreadsheet_id, data):

	results = data[0]
	exceptions = data[1]
	report = data[2]

	service.spreadsheets().values().update(
		spreadsheetId = spreadsheet_id,
		range = results['range'],
		body = {'values': results['values']},
		valueInputOption = 'USER_ENTERED'
	).execute()

	service.spreadsheets().values().append(
		spreadsheetId = spreadsheet_id,
		range = exceptions['range'],
		body = {'values': exceptions['values']},
		valueInputOption = 'USER_ENTERED'
	).execute()

	service.spreadsheets().values().append(
		spreadsheetId = spreadsheet_id,
		range = report['range'],
		# XXX Fixme, should not have to add list
		body = {'values': [report['values']]},
		valueInputOption = 'USER_ENTERED'
	).execute()


def process_all_file_entries(work_sheet, pivot_file, prev_data, existing_files):

	result = []
	pivot_file_count = len(pivot_file)
	blank_row = [''] * 11

	tqdmIterator = tqdm(pivot_file)

	for index, file in enumerate(tqdmIterator):
		tqdmIterator.set_description(work_sheet)
		file += [''] * (2 - len(file))
		media_id = file[0].strip()
		if media_id != '':
			side = normalized_side(file[1])
			key = media_id + '-' + side
			existing = pop_existing_files(existing_files, key)
			# check if already exists
			result.append(add_new_files(prev_data, index, existing))
		else:
			result.append(blank_row)

	# generate reports

	report = generate_report(work_sheet, result)
	exceptions = prepare_exceptions(work_sheet, existing_files)
	return result, exceptions, report

def generate_report(work_sheet, data):
	# Total files, passing files
	total_files = 0
	passing_files = 0
	total_time = 0
	average_time = 0
	for datum in data:
		total_files += 1
		if is_passing_file(datum):
			passing_files += 1

			total_time += get_seconds(datum[5])
		# print datum


	return [
		work_sheet,
		# all inventory files
		total_files,
		# files with no detectable error
		passing_files,
		# all digitized time
		get_duration_from_seconds(total_time),
		# average time per recording
		get_duration_from_seconds(total_time / passing_files)
	]

def get_duration_from_seconds(total_seconds):
	hours, minutes = divmod(total_seconds, 60*60)
	minutes, seconds = divmod(minutes, 60)
	return "{0:02d}".format(int(hours)) + ":" + "{0:02d}".format(int(minutes)) + ":" + "{0:02.2f}".format(seconds)


def get_seconds(time):
	# 01:05:45.18
	parts = time.split(':')
	seconds = float(parts[2])
	seconds += float(parts[1]) * 60
	seconds += float(parts[0]) * 60 * 60
	return seconds

def is_passing_file(datum):
	blank_row = ['', '', '', '', '', '', '', '', '', '[]', 0]
	# check expected values
	if (datum == blank_row or
		datum[2] != '2' or
		datum[3] != '96000' or
		datum[4] != '24-bit' or
		datum[7] != '4.61M' or
		datum[8] != '24-bit Signed Integer PCM' or
		datum[10] != 1
		):
		return False

	return True

def prepare_exceptions(work_sheet, files):
	result = []
	entries = flatten_dictionary(files)
	for entry in entries:
		for file in entry:
			line = get_file_information(file)
			name = (file['id'] + '-'
					+ file['side'] + '-'
					+ file['dig'] + '-'
					+ file['take'] + '.wav')
			to_append = [work_sheet, file["path"], name]


			result.append(to_append + line)
	return result


def flatten_dictionary(dictionary):
	result = []
	for key in dictionary:
		result.append(dictionary[key])
	return result


def add_new_files(prev_data, index, existing):

	result = existing

	if index < len(prev_data):
		prev_datum = json.loads(str(prev_data[index][0]))
		for prev_file in prev_datum:
			found = False
			for file in result:
				if file["md5"] == prev_file["md5"]:
					found = True
					break

			if not found:
				result.append(prev_file)

	result_count = len(result)

	information_row = [''] * 9
	matching_md5 = ""
	if result_count == 1:
		information_row = get_file_information(result[0])
	elif result_count > 1:
		for index, file_info in enumerate(result):
			scout_index = index + 1
			this_md5 = file_info["md5"]
			while scout_index < len(result):
				if this_md5 == result[scout_index]["md5"]:
					matching_md5 = this_md5
					break
				scout_index += 1


	return information_row + [json.dumps(result), result_count, matching_md5];


def get_file_information(file):
	result = [''] * 9
	result[0] = file['id'] + '-' + file['side'] + '.wav'
	result[1] = file['notes']
	result[2] = file['channels']
	result[3] = file['sample_rate']
	result[4] = file['precision']
	result[5] = file['duration']
	result[6] = file['size']
	result[7] = file['bitrate']
	result[8] = file['encoding']
	return result


def get_service(flags):
	credentials = get_credentials(flags)
	http = credentials.authorize(httplib2.Http())
	discovery_url = ('https://sheets.googleapis.com/$discovery/rest?'
					 'version=v4')
	service = discovery.build(
		'sheets', 'v4', http=http, discoveryServiceUrl=discovery_url
	)

	return service


def get_inventory_data(service, spreadsheet_id, range):

	result = service.spreadsheets().values().get(
        spreadsheetId=spreadsheet_id, range=range).execute()
	values = result.get('values', [])

	if not values:
		return []
	else:
		return values


# from https://developers.google.com/sheets/api/quickstart/python
def get_credentials(flags):

	SCOPES = 'https://www.googleapis.com/auth/spreadsheets'
	CLIENT_SECRET_FILE = 'client_secret.json'
	APPLICATION_NAME = 'Google Sheets API Python Quickstart'

	home_dir = os.path.expanduser('~')
	credential_dir = os.path.join(home_dir, '.credentials')
	if not os.path.exists(credential_dir):
		os.makedirs(credential_dir)
	credential_path = os.path.join(credential_dir,
		'sheets.googleapis.com-python-quickstart.json')

	store = Storage(credential_path)
	credentials = store.get()
	if not credentials or credentials.invalid:
		flow = client.flow_from_clientsecrets(CLIENT_SECRET_FILE, SCOPES)
		flow.user_agent = APPLICATION_NAME
		if flags:
			credentials = tools.run_flow(flow, store, flags)
		else: # Needed only for compatibility with Python 2.6
			credentials = tools.run(flow, store)
		print('Storing credentials to ' + credential_path)
	return credentials


def get_file_dictionary(file_path):
	with open(file_path) as csv_file:
		reader = csv.DictReader(csv_file)
		return list(reader)


def get_existing_files(processed_file):
	result = {}
	for file in processed_file:
		if file['id'] != None and file['side'] != None:
			key = file['id'] + '-' + file['side']
			if key not in result:
				result[key] = []
			result[key].append(file)
		else:
			print file
	return result


def normalized_side(x):
	return {
		'b': 'b',
		'2': 'b',
	}.get(x.strip().lower(), 'a')


def pop_existing_files(existing_files, key):
	if key not in existing_files:
		return []
	else:
		return existing_files.pop(key)


if __name__ == '__main__':
	main()